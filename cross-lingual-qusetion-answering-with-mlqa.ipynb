{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets transformers","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:42:59.006050Z","iopub.execute_input":"2024-10-15T14:42:59.006359Z","iopub.status.idle":"2024-10-15T14:43:12.243351Z","shell.execute_reply.started":"2024-10-15T14:42:59.006325Z","shell.execute_reply":"2024-10-15T14:43:12.242113Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\nlangs = [ \"de\", \"en\", \"es\", \"hi\"]\ntranslate_langs = [ \"de\",  \"es\"]\nlangs_test = []\nlangs_translate_test = []\nlangs_translate_train = [ \"de.de\", \"en.en\", \"es.es\", \"hi.hi\"]\nlangs_translate_train_all = []\n\nmlqa = {}\n\nfor lang1 in langs:\n    for lang2 in langs:\n        mlqa[f\"{lang1}.{lang2}\"] = load_dataset(\"mlqa\", f\"mlqa.{lang1}.{lang2}\")\n        langs_test.append(f\"{lang1}.{lang2}\")\n\nfor lang in translate_langs:\n    mlqa[f\"translate-train.{lang}\"] = load_dataset(\"mlqa\", f\"mlqa-translate-train.{lang}\")\n    mlqa[f\"translate-test.{lang}\"] = load_dataset(\"mlqa\", f\"mlqa-translate-test.{lang}\")\n    langs_translate_test.append(f\"translate-test.{lang}\")\n    langs_translate_train_all.append(f\"translate-train.{lang}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:43:17.909190Z","iopub.execute_input":"2024-10-15T14:43:17.909974Z","iopub.status.idle":"2024-10-15T14:45:06.541552Z","shell.execute_reply.started":"2024-10-15T14:43:17.909911Z","shell.execute_reply":"2024-10-15T14:45:06.540815Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"mlqa.py:   0%|          | 0.00/8.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"836a24cedfcd465f97a10e16b8c344ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/34.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ec077ded2384ecbb9070a615cf8ff82"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for mlqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mlqa.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/75.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"731d259fc99a4f3fba77a09ae67b4453"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4517 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1efe49ab2ec24aa99c857c7714dace5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/512 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8f8636b9f0c48719a194792a564017e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4517 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ecdd6907729431f9133cc992de092cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/512 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07da0a975ff9475196bcdcbcd485114b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1776 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae00820c0e804680a83ef6424f21e9a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/196 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4e09d29b1cd4a6ba037226fad814d70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1430 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb05851aa4434483a43c985baf143cdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/163 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5078575ead84d668da8bd1676f73ffc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4517 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5df6879b50a451d939e1970a566c293"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/512 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e58a38ac5bef4b09a6f83c19b16158ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11590 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1450d75cb25949d19dc2485937c11671"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1148 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dae05735d54b4105b0de1f65106dd8f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5253 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc314efc6d284d85b86fe79a34c06184"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1d480e2438e49baac598def33e8ed7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4918 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"435dbe7d4ac8426a9cd4f4f726088463"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/507 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8384d15e414948c69b8cc498d1873185"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1776 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c21bc6cc6224d84877aac499015ef10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/196 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"912b915c088f4df1a021e96a3998dba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5253 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80fb20983b264df0b77415cfbefd9932"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4646148eb698429496ad173661f39a62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5253 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f59a042d20c446bb455fa6bb1d6e0bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdcd7d1ccea24387aac9c972309da7bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1723 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd4b914cef364061b9c92f70b636a282"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/187 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cb5821601a84ffdb1235cc4dcf964e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1430 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"910446c39c614624ac25e9879bd3bccd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/163 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d82bbf6522d44164b63340ecdda57196"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4918 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fdd6abfd2ea421e90c198ae51686846"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/507 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d95cad823b1c4dadb8543ea6897f031a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1723 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c29a8a6c0e34ce58d6d34ed63024b6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/187 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f72ab6eb12b4e65b978322476e05182"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4918 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6687c5218be04d75b2d88bcdef27f2d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/507 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72a5636437d84ee9a94f6f17ed6d5db3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/63.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7b87b39c10d4d49b5be254e36368764"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/80069 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1deaada9ca454fa2b1b36936b08dacf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/9927 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f617f4d34f9c4660b0e73158ded7ef17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/10.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"494bcf39dc2a46b19d05b539d468abb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4517 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db6dbb985acb4dd0ad27eb9aa2bdd3c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/81810 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18e1a195e344477aae41e6d674b2c541"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10123 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6d0ae8bda9b4f0b955d56b078490433"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5253 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4d6a93208424ec3acc7e1d979d7382b"}},"metadata":{}}]},{"cell_type":"code","source":"mlqa","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:45:48.753852Z","iopub.execute_input":"2024-10-15T14:45:48.754265Z","iopub.status.idle":"2024-10-15T14:45:48.763258Z","shell.execute_reply.started":"2024-10-15T14:45:48.754226Z","shell.execute_reply":"2024-10-15T14:45:48.762329Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'de.de': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 4517\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 512\n     })\n }),\n 'de.en': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 4517\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 512\n     })\n }),\n 'de.es': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 1776\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 196\n     })\n }),\n 'de.hi': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 1430\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 163\n     })\n }),\n 'en.de': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 4517\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 512\n     })\n }),\n 'en.en': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 11590\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 1148\n     })\n }),\n 'en.es': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 5253\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 500\n     })\n }),\n 'en.hi': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 4918\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 507\n     })\n }),\n 'es.de': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 1776\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 196\n     })\n }),\n 'es.en': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 5253\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 500\n     })\n }),\n 'es.es': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 5253\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 500\n     })\n }),\n 'es.hi': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 1723\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 187\n     })\n }),\n 'hi.de': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 1430\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 163\n     })\n }),\n 'hi.en': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 4918\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 507\n     })\n }),\n 'hi.es': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 1723\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 187\n     })\n }),\n 'hi.hi': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 4918\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 507\n     })\n }),\n 'translate-train.de': DatasetDict({\n     train: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 80069\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 9927\n     })\n }),\n 'translate-test.de': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 4517\n     })\n }),\n 'translate-train.es': DatasetDict({\n     train: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 81810\n     })\n     validation: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 10123\n     })\n }),\n 'translate-test.es': DatasetDict({\n     test: Dataset({\n         features: ['context', 'question', 'answers', 'id'],\n         num_rows: 5253\n     })\n })}"},"metadata":{}}]},{"cell_type":"code","source":"mlqa[\"en.es\"][\"test\"][0]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:46:01.765042Z","iopub.execute_input":"2024-10-15T14:46:01.765440Z","iopub.status.idle":"2024-10-15T14:46:01.777299Z","shell.execute_reply.started":"2024-10-15T14:46:01.765402Z","shell.execute_reply":"2024-10-15T14:46:01.776218Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'context': 'In 1994, five unnamed civilian contractors and the widows of contractors Walter Kasza and Robert Frost sued the USAF and the United States Environmental Protection Agency. Their suit, in which they were represented by George Washington University law professor Jonathan Turley, alleged they had been present when large quantities of unknown chemicals had been burned in open pits and trenches at Groom. Biopsies taken from the complainants were analyzed by Rutgers University biochemists, who found high levels of dioxin, dibenzofuran, and trichloroethylene in their body fat. The complainants alleged they had sustained skin, liver, and respiratory injuries due to their work at Groom, and that this had contributed to the deaths of Frost and Kasza. The suit sought compensation for the injuries they had sustained, claiming the USAF had illegally handled toxic materials, and that the EPA had failed in its duty to enforce the Resource Conservation and Recovery Act (which governs handling of dangerous materials). They also sought detailed information about the chemicals to which they were allegedly exposed, hoping this would facilitate the medical treatment of survivors. Congressman Lee H. Hamilton, former chairman of the House Intelligence Committee, told 60 Minutes reporter Lesley Stahl, \"The Air Force is classifying all information about Area 51 in order to protect themselves from a lawsuit.\"',\n 'question': '¿Quién analizó las biopsias?',\n 'answers': {'answer_start': [457],\n  'text': ['Rutgers University biochemists']},\n 'id': 'a4968ca8a18de16aa3859be760e43dbd3af3fce9'}"},"metadata":{}}]},{"cell_type":"code","source":"mlqa[\"en.es\"][\"validation\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:46:04.716134Z","iopub.execute_input":"2024-10-15T14:46:04.716537Z","iopub.status.idle":"2024-10-15T14:46:04.724130Z","shell.execute_reply.started":"2024-10-15T14:46:04.716500Z","shell.execute_reply":"2024-10-15T14:46:04.723028Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'context': 'Pappataci fever is prevalent in the subtropical zone of the Eastern Hemisphere between 20°N and 45°N, particularly in Southern Europe, North Africa, the Balkans, Eastern Mediterranean, Iraq, Iran, Pakistan, Afghanistan and India.The disease is transmitted by the bites of phlebotomine sandflies of the Genus Phlebotomus, in particular, Phlebotomus papatasi, Phlebotomus perniciosus and Phlebotomus perfiliewi. The sandfly becomes infected when biting an infected human in the period between 48 hours before the onset of fever and 24 hours after the end of the fever, and remains infected for its lifetime. Besides this horizontal virus transmission from man to sandfly, the virus can be transmitted in insects transovarially, from an infected female sandfly to its offspring.Pappataci fever is seldom recognised in endemic populations because it is mixed with other febrile illnesses of childhood, but it is more well-known among immigrants and military personnel from non-endemic regions.',\n 'question': 'Una infección de moscas de la arena ¿desaparece con el tiempo?',\n 'answers': {'answer_start': [571],\n  'text': ['remains infected for its lifetime']},\n 'id': '569666f4dc3983dab5624e989212c1d9d0cd1798'}"},"metadata":{}}]},{"cell_type":"code","source":"mlqa[\"translate-train.es\"][\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:46:07.803759Z","iopub.execute_input":"2024-10-15T14:46:07.804177Z","iopub.status.idle":"2024-10-15T14:46:07.811332Z","shell.execute_reply.started":"2024-10-15T14:46:07.804139Z","shell.execute_reply":"2024-10-15T14:46:07.810390Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'context': 'Arquitectónico, la escuela tiene un carácter católico. sobre la cúpula de oro del edificio principal es una estatua de oro de la Virgen María. inmediatamente frente al edificio principal y frente a ella, es una estatua de cobre de Cristo con los brazos levantado con la leyenda venite ad me omnes. junto al edificio principal se encuentra la Basílica del sagrado corazón. Inmediatamente detrás de la Basílica se encuentra la gruta, un lugar de oración y reflexión de Marian. Se trata de una réplica de la gruta en Lourdes, Francia, donde la Virgen María supuestamente apareció a saint bernadette soubirous en 1858. Al final de la unidad principal (y en una línea directa que conecta a través de 3 estatuas y la cúpula de oro), es una simple y moderna estatua de piedra de María.',\n 'question': 'A quién presuntamente apareció la Virgen María en 1858 en Lourdes Francia?',\n 'answers': {'answer_start': [575], 'text': ['Santa Bernadette Soubirous']},\n 'id': '5733be284776f41900661182'}"},"metadata":{}}]},{"cell_type":"code","source":"mlqa[\"translate-train.es\"][\"validation\"][0]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:46:10.764073Z","iopub.execute_input":"2024-10-15T14:46:10.764744Z","iopub.status.idle":"2024-10-15T14:46:10.771765Z","shell.execute_reply.started":"2024-10-15T14:46:10.764701Z","shell.execute_reply":"2024-10-15T14:46:10.770733Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'context': \"Super Bowl 50 fue un juego de fútbol americano para determinar el campeón de la liga nacional de fútbol (NFL) para la temporada 2015 la conferencia de fútbol americano (AFC) campeón DENVER BRONCOS derrotó a la conferencia nacional de fútbol (NFC) Campeona Carolina Panthers 24-10 para ganar su tercer título de super bowl. El juego se jugó el 7 de febrero de 2016, en el estadio Levi ' s en la zona de la bahía de San Francisco en santa clara, California. Como este fue el 50º super bowl, la liga hizo hincapié en el aniversario de oro con varias iniciativas temáticas de oro, así como suspender temporalmente la tradición de nombrar cada juego de super bowl con números romanos (bajo el cual el juego habría sido conocido como super bowl l), de modo que el logotipo podría característica de forma prominente los números árabes 50.\",\n 'question': 'Qué equipo de la nfl representó a la afc en super bowl 50?',\n 'answers': {'answer_start': [182, 182, 182],\n  'text': ['DENVER BRONCOS', 'DENVER BRONCOS', 'DENVER BRONCOS']},\n 'id': '56be4db0acb8001400a502ec'}"},"metadata":{}}]},{"cell_type":"code","source":"mlqa[\"translate-test.es\"][\"test\"][0]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:46:13.005274Z","iopub.execute_input":"2024-10-15T14:46:13.005672Z","iopub.status.idle":"2024-10-15T14:46:13.012500Z","shell.execute_reply.started":"2024-10-15T14:46:13.005634Z","shell.execute_reply":"2024-10-15T14:46:13.011690Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'context': 'After the eruption, the emissions of pyroclastic material that occurred from the gap created by the collapse were mostly magmatic origin, and in lower proportion of fragments of pre-existing volcanic rocks. The resulting deposits formed a fan-shaped structures that followed a pattern of leaves, languages and overlapping lobes. During the eruption of may 18, there were at least 17 emissions of pyroclastic flow separated over time, whose volumes of aggregation were around 208 million m3.',\n 'question': 'What was the appearance of the deposits that left that landslide?',\n 'answers': {'answer_start': [-1],\n  'text': ['A Fan-shaped structures that followed a pattern of leaves, languages and lobes']},\n 'id': 'b77c037b331e06542272669766df3b9515366b57'}"},"metadata":{}}]},{"cell_type":"code","source":"pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:46:16.004344Z","iopub.execute_input":"2024-10-15T14:46:16.004734Z","iopub.status.idle":"2024-10-15T14:46:28.354957Z","shell.execute_reply.started":"2024-10-15T14:46:16.004696Z","shell.execute_reply":"2024-10-15T14:46:28.353781Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"squad = load_dataset(\"squad\")","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:46:36.707049Z","iopub.execute_input":"2024-10-15T14:46:36.707429Z","iopub.status.idle":"2024-10-15T14:46:47.108797Z","shell.execute_reply.started":"2024-10-15T14:46:36.707394Z","shell.execute_reply":"2024-10-15T14:46:47.107810Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"877ae4d4fb1b445db1d858b5bd21382b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b493331063f40aaa07ab351b96ca2d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3813fde6bee1493db54d943ab9f1b77f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d71dd6f1d8d6429d9e6fa376d12c2f3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ca02ed98cfb4ab8a0a09e955067e3c3"}},"metadata":{}}]},{"cell_type":"code","source":"squad\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:46:54.675217Z","iopub.execute_input":"2024-10-15T14:46:54.675933Z","iopub.status.idle":"2024-10-15T14:46:54.681878Z","shell.execute_reply.started":"2024-10-15T14:46:54.675892Z","shell.execute_reply":"2024-10-15T14:46:54.680898Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 87599\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 10570\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"squad[\"train\"][0]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:46:59.420178Z","iopub.execute_input":"2024-10-15T14:46:59.420570Z","iopub.status.idle":"2024-10-15T14:46:59.428271Z","shell.execute_reply.started":"2024-10-15T14:46:59.420525Z","shell.execute_reply":"2024-10-15T14:46:59.427263Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'id': '5733be284776f41900661182',\n 'title': 'University_of_Notre_Dame',\n 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"},"metadata":{}}]},{"cell_type":"code","source":"squad[\"validation\"][0]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:47:10.169370Z","iopub.execute_input":"2024-10-15T14:47:10.169753Z","iopub.status.idle":"2024-10-15T14:47:10.177041Z","shell.execute_reply.started":"2024-10-15T14:47:10.169716Z","shell.execute_reply":"2024-10-15T14:47:10.176061Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'id': '56be4db0acb8001400a502ec',\n 'title': 'Super_Bowl_50',\n 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n  'answer_start': [177, 177, 177]}}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_name = \"bert-base-multilingual-cased\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:47:15.789564Z","iopub.execute_input":"2024-10-15T14:47:15.789963Z","iopub.status.idle":"2024-10-15T14:47:24.663666Z","shell.execute_reply.started":"2024-10-15T14:47:15.789913Z","shell.execute_reply":"2024-10-15T14:47:24.662618Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e031a04ca9c14572b80c281595df8112"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d50815392264cb19a56298e74f1bf38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b87d7bd464f44539ae6b9a7462c6e255"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d309c36f67440cc845d1ae52c76cdfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba82db7735ef4da5b6e2ed75f4eb4df0"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"max_length = 384 # The maximum length of a feature (question and context)\ndoc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:47:42.292358Z","iopub.execute_input":"2024-10-15T14:47:42.293413Z","iopub.status.idle":"2024-10-15T14:47:42.297668Z","shell.execute_reply.started":"2024-10-15T14:47:42.293371Z","shell.execute_reply":"2024-10-15T14:47:42.296644Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def prepare_train_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Let's label those examples!\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                # Note: we could go after the last offset if the answer is the last word (edge case).\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:47:44.534799Z","iopub.execute_input":"2024-10-15T14:47:44.535193Z","iopub.status.idle":"2024-10-15T14:47:44.549061Z","shell.execute_reply.started":"2024-10-15T14:47:44.535155Z","shell.execute_reply":"2024-10-15T14:47:44.548017Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"squad_train = squad.map(prepare_train_features, batched=True, \n                            remove_columns=squad[\"train\"].column_names)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:47:46.614574Z","iopub.execute_input":"2024-10-15T14:47:46.614968Z","iopub.status.idle":"2024-10-15T14:48:49.886586Z","shell.execute_reply.started":"2024-10-15T14:47:46.614911Z","shell.execute_reply":"2024-10-15T14:48:49.885598Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36be8103fa174ac79e78bc809496a80c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9d3980fe56844228f551f20c1ccd801"}},"metadata":{}}]},{"cell_type":"code","source":"def prepare_validation_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # We keep the example_id that gave us this feature and we will store the offset mappings.\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1\n        \n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n\n        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n        # position is part of the context or not.\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:48:52.824431Z","iopub.execute_input":"2024-10-15T14:48:52.824832Z","iopub.status.idle":"2024-10-15T14:48:52.833993Z","shell.execute_reply.started":"2024-10-15T14:48:52.824793Z","shell.execute_reply":"2024-10-15T14:48:52.832880Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"squad_eval = squad[\"validation\"].map(prepare_validation_features, batched=True, \n                                          remove_columns=squad[\"validation\"].column_names)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:48:55.406748Z","iopub.execute_input":"2024-10-15T14:48:55.407171Z","iopub.status.idle":"2024-10-15T14:49:04.249973Z","shell.execute_reply.started":"2024-10-15T14:48:55.407133Z","shell.execute_reply":"2024-10-15T14:49:04.248975Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"075d66d3571c47d3a8d0222b05984891"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n\nmodel_name = \"bert-base-multilingual-cased\"\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:49:08.293682Z","iopub.execute_input":"2024-10-15T14:49:08.294083Z","iopub.status.idle":"2024-10-15T14:49:25.794292Z","shell.execute_reply.started":"2024-10-15T14:49:08.294046Z","shell.execute_reply":"2024-10-15T14:49:25.793414Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"938759e169fa43beaf571d960882db08"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import DefaultDataCollator\n\ndata_collator = DefaultDataCollator()","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:49:28.227789Z","iopub.execute_input":"2024-10-15T14:49:28.228778Z","iopub.status.idle":"2024-10-15T14:49:28.233085Z","shell.execute_reply.started":"2024-10-15T14:49:28.228734Z","shell.execute_reply":"2024-10-15T14:49:28.232012Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import torch\n\nbatch_size = 16\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nargs = TrainingArguments(\n    \"bert-base-multilingual-cased-squad\",\n    eval_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    # push_to_hub=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:49:30.592156Z","iopub.execute_input":"2024-10-15T14:49:30.592556Z","iopub.status.idle":"2024-10-15T14:49:30.671573Z","shell.execute_reply.started":"2024-10-15T14:49:30.592518Z","shell.execute_reply":"2024-10-15T14:49:30.670782Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model.to(device),\n    args,\n    train_dataset=squad_train[\"train\"],\n    eval_dataset=squad_train[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:49:33.022825Z","iopub.execute_input":"2024-10-15T14:49:33.023673Z","iopub.status.idle":"2024-10-15T14:49:34.632443Z","shell.execute_reply.started":"2024-10-15T14:49:33.023614Z","shell.execute_reply":"2024-10-15T14:49:34.631496Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:49:40.467926Z","iopub.execute_input":"2024-10-15T14:49:40.468361Z","iopub.status.idle":"2024-10-15T14:49:40.472789Z","shell.execute_reply.started":"2024-10-15T14:49:40.468323Z","shell.execute_reply":"2024-10-15T14:49:40.471861Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:49:41.410818Z","iopub.execute_input":"2024-10-15T14:49:41.411883Z","iopub.status.idle":"2024-10-15T14:49:41.416393Z","shell.execute_reply.started":"2024-10-15T14:49:41.411827Z","shell.execute_reply":"2024-10-15T14:49:41.415330Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n\nmodel_name = \"salti/bert-base-multilingual-cased-finetuned-squad\"\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndata_collator = DefaultDataCollator()\n\nbatch_size = 16\ntraining_args = TrainingArguments(\n    output_dir=\"bert-base-multilingual-cased-finetuned-squad\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=None,\n    eval_dataset=None,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:49:45.272612Z","iopub.execute_input":"2024-10-15T14:49:45.273027Z","iopub.status.idle":"2024-10-15T14:49:54.924649Z","shell.execute_reply.started":"2024-10-15T14:49:45.272989Z","shell.execute_reply":"2024-10-15T14:49:54.923627Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/822 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9a2d73a7d564c999bbcebcb6804e693"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/709M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4131e300cfd43de876590031c284d31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/264 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c17d805b23864546bfe6c38e60a02254"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"802affc317ed4f00bb5ed28b6fa7dcb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f25f3f10ac19478b80d7b91dc28d72fc"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"raw_predictions = trainer.predict(squad_eval)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:49:59.341349Z","iopub.execute_input":"2024-10-15T14:49:59.341782Z","iopub.status.idle":"2024-10-15T14:52:31.339515Z","shell.execute_reply.started":"2024-10-15T14:49:59.341746Z","shell.execute_reply":"2024-10-15T14:52:31.338628Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113275700000081, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9b3d8e60aba4920bfdd1e027d0eb12f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241015_145227-64wzfd9m</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/iiith-smai/huggingface/runs/64wzfd9m' target=\"_blank\">bert-base-multilingual-cased-finetuned-squad</a></strong> to <a href='https://wandb.ai/iiith-smai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/iiith-smai/huggingface' target=\"_blank\">https://wandb.ai/iiith-smai/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/iiith-smai/huggingface/runs/64wzfd9m' target=\"_blank\">https://wandb.ai/iiith-smai/huggingface/runs/64wzfd9m</a>"},"metadata":{}}]},{"cell_type":"code","source":"squad_eval.set_format(type=squad_eval.format[\"type\"], \n                      columns=list(squad_eval.features.keys()))","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:52:36.764835Z","iopub.execute_input":"2024-10-15T14:52:36.765260Z","iopub.status.idle":"2024-10-15T14:52:36.772231Z","shell.execute_reply.started":"2024-10-15T14:52:36.765213Z","shell.execute_reply":"2024-10-15T14:52:36.771337Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport collections\nimport numpy as np\n\ndef postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n    all_start_logits, all_end_logits = raw_predictions\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    # The dictionaries we have to fill.\n    predictions = collections.OrderedDict()\n\n    # Logging.\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in enumerate(tqdm(examples)):\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n\n        valid_answers = []\n        \n        context = example[\"context\"]\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n            # failure.\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        # Let's pick our final answer: the best one\n        predictions[example[\"id\"]] = best_answer[\"text\"]\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:52:39.411253Z","iopub.execute_input":"2024-10-15T14:52:39.411956Z","iopub.status.idle":"2024-10-15T14:52:39.427255Z","shell.execute_reply.started":"2024-10-15T14:52:39.411897Z","shell.execute_reply":"2024-10-15T14:52:39.426289Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"final_predictions = postprocess_qa_predictions(squad[\"validation\"], squad_eval, raw_predictions.predictions)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:52:42.949021Z","iopub.execute_input":"2024-10-15T14:52:42.949912Z","iopub.status.idle":"2024-10-15T14:53:28.042663Z","shell.execute_reply.started":"2024-10-15T14:52:42.949872Z","shell.execute_reply":"2024-10-15T14:53:28.041726Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Post-processing 10570 example predictions split into 10851 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10570 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ce9e8a806dd4257b29676d2ce985881"}},"metadata":{}}]},{"cell_type":"code","source":"from evaluate import load\nsquad_metric = load(\"squad\")\n\nformatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\nreferences = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in squad[\"validation\"]]\nsquad_metric.compute(predictions=formatted_predictions, references=references)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:53:33.115639Z","iopub.execute_input":"2024-10-15T14:53:33.116055Z","iopub.status.idle":"2024-10-15T14:53:40.043484Z","shell.execute_reply.started":"2024-10-15T14:53:33.116018Z","shell.execute_reply":"2024-10-15T14:53:40.042499Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"527772dda7ef4baf8de076740355b37c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce7da6eb10754889bbdebed908a45295"}},"metadata":{}},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"{'exact_match': 81.90160832544939, 'f1': 89.121876471452}"},"metadata":{}}]},{"cell_type":"code","source":"\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments, Trainer, DefaultDataCollator\n\nmodel_name = \"salti/bert-base-multilingual-cased-finetuned-squad\"\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndata_collator = DefaultDataCollator()\n\nbatch_size = 16\ntraining_args = TrainingArguments(\n    output_dir=\"bert-base-multilingual-cased-finetuned-squad\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=None,\n    eval_dataset=None,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:53:43.979678Z","iopub.execute_input":"2024-10-15T14:53:43.980096Z","iopub.status.idle":"2024-10-15T14:53:45.118136Z","shell.execute_reply.started":"2024-10-15T14:53:43.980050Z","shell.execute_reply":"2024-10-15T14:53:45.117233Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from collections import defaultdict\n\nmlqa_prep = defaultdict(dict)\n\ndef map_datasets(langs, split, prepare_features):\n    for lang in langs:\n        mlqa_prep[lang][split] = mlqa[lang][split].map(prepare_features, batched=True, \n                                    remove_columns=mlqa[lang][split].column_names)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:53:50.884680Z","iopub.execute_input":"2024-10-15T14:53:50.885881Z","iopub.status.idle":"2024-10-15T14:53:50.892546Z","shell.execute_reply.started":"2024-10-15T14:53:50.885824Z","shell.execute_reply":"2024-10-15T14:53:50.891701Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"split = \"test\"\nmap_datasets(langs_test, split, prepare_validation_features)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:53:53.391408Z","iopub.execute_input":"2024-10-15T14:53:53.392348Z","iopub.status.idle":"2024-10-15T14:55:05.795737Z","shell.execute_reply.started":"2024-10-15T14:53:53.392304Z","shell.execute_reply":"2024-10-15T14:55:05.794754Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4517 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb869ebf57924dcdb27f08125ac7f23e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4517 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30dd98069d28401092b16f8d306984ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1776 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1f624bec32647038e3b444c42e6d638"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1430 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2241a0e0e2fb48d684a44e83886d911e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4517 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0828c1f33e346b7829fea6a05031da7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11590 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"910ac9a58d9e4d829f1234f6d18e50d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5253 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12a23d14071e4999b7d3062300602d03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4918 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"623c47b6a3f248d2b1b4002202765c23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1776 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ab8754711444a25847bd39a9d3734e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5253 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57eeab91ab9d4666841cf2a10035bc81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5253 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"780736141b9c4fb593b322dd340fc418"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1723 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"187fcb117ff147b388d5ed272f68635c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1430 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a23d2c16bcdb490da4917d9baa3539df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4918 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ca2acc5279f4b7380e241b5f87e2e94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1723 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bf25d191c0c4039a0ade6a2c4e32dc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4918 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e2d87c0fb3746a5bb2af97b1cb98f1d"}},"metadata":{}}]},{"cell_type":"code","source":"def compute_results(langs, split):\n    results = {}\n    for lang in langs:\n        # We can grab the predictions for all features by using the method\n        raw_predictions = trainer.predict(mlqa_prep[lang][split])\n\n        # example_id and offset_mapping which we will need for our post-processing\n        mlqa_prep[lang][split].set_format(type=mlqa_prep[lang][split].format[\"type\"], \n                        columns=list(mlqa_prep[lang][split].features.keys()))\n        \n        # And we can apply our post-processing function to our raw predictions\n        final_predictions = postprocess_qa_predictions(mlqa[lang][split], mlqa_prep[lang][split], raw_predictions.predictions)\n\n        # We just need to format predictions and labels a bit as it expects a list of dictionaries and not one big dictionary.\n        formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n        references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in mlqa[lang][split]]\n        results[lang] = squad_metric.compute(predictions=formatted_predictions, references=references)\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:55:13.989687Z","iopub.execute_input":"2024-10-15T14:55:13.990433Z","iopub.status.idle":"2024-10-15T14:55:13.999416Z","shell.execute_reply.started":"2024-10-15T14:55:13.990389Z","shell.execute_reply":"2024-10-15T14:55:13.998399Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"results_zero_shot_mbert = compute_results(langs_test, split)\nprint(results_zero_shot_mbert)\n     ","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:55:15.914393Z","iopub.execute_input":"2024-10-15T14:55:15.914805Z","iopub.status.idle":"2024-10-15T15:18:39.407156Z","shell.execute_reply.started":"2024-10-15T14:55:15.914768Z","shell.execute_reply":"2024-10-15T15:18:39.406152Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4517 example predictions split into 5242 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4517 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f6d2eb7270d41eebdc9e7384307f69d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4517 example predictions split into 5232 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4517 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8932116743aa4e69864090e43c836b7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1776 example predictions split into 2046 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1776 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"518a7f95cffb45a48c2752248683fbb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1430 example predictions split into 1604 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e7e067817f04c63a96070eb4b7bc3c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4517 example predictions split into 5552 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4517 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"563bd048219640839aba99da3b4ccff4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 11590 example predictions split into 14706 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/11590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f87123476e143f6a53dd73ce58d8d3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 5253 example predictions split into 6585 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5253 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb942497f86f4d9b8b31b895996914ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4918 example predictions split into 6360 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4918 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01e6d66ba4ad4ae7a3468a7cbbde625c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1776 example predictions split into 1833 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1776 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eac58d48ddc4cdfaaf048e82262fdc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 5253 example predictions split into 5451 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5253 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9d2c29ece7b4b7fa5f8c6fd6025b751"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 5253 example predictions split into 5456 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5253 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66ec3fe41d844a78aafa272eacd7af74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1723 example predictions split into 1790 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1723 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16c0553214284c9ab6a4a698ee82c878"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1430 example predictions split into 2007 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cab653be61c47b1affd2f4d2aed01da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4918 example predictions split into 7359 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4918 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bdbcd06e04446ff97f1dfcfab2cb4f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1723 example predictions split into 2489 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1723 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1acd0642cacb43f4914971f88b6d798c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4918 example predictions split into 7500 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4918 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a392dd6390742159ed7d1bf2aae4a8d"}},"metadata":{}},{"name":"stdout","text":"{'de.de': {'exact_match': 43.74584901483286, 'f1': 59.39314267422581}, 'de.en': {'exact_match': 46.734558335178214, 'f1': 62.361949176983636}, 'de.es': {'exact_match': 40.990990990990994, 'f1': 56.37397727738263}, 'de.hi': {'exact_match': 21.53846153846154, 'f1': 34.05070095047693}, 'en.de': {'exact_match': 52.46845251272969, 'f1': 66.43219113113172}, 'en.en': {'exact_match': 67.02329594477999, 'f1': 80.30441656441819}, 'en.es': {'exact_match': 52.750809061488674, 'f1': 67.3820769943422}, 'en.hi': {'exact_match': 26.270841805612037, 'f1': 39.268153627134296}, 'es.de': {'exact_match': 40.2027027027027, 'f1': 60.59163050770253}, 'es.en': {'exact_match': 46.3925375975633, 'f1': 66.93311228899755}, 'es.es': {'exact_match': 43.61317342470969, 'f1': 64.87841269590633}, 'es.hi': {'exact_match': 20.08125362739408, 'f1': 36.17424370265638}, 'hi.de': {'exact_match': 33.84615384615385, 'f1': 47.54571494149746}, 'hi.en': {'exact_match': 37.10858072387149, 'f1': 52.90239979877026}, 'hi.es': {'exact_match': 29.077190946024377, 'f1': 43.725366437185066}, 'hi.hi': {'exact_match': 30.012200081333877, 'f1': 46.20710204954478}}\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ndef results_df(results_dict, model):\n    F1colname = \"F1_\" + model\n    EMcolname = \"EM_\" + model\n    dict_results = defaultdict(list)\n    for lang, scores in results_dict.items():\n        dict_results[\"lang\"].append(lang)\n        dict_results[F1colname].append(scores['f1'])\n        dict_results[EMcolname].append(scores['exact_match'])\n\n    avg_f1 = np.average(dict_results[F1colname])\n    avg_em = np.average(dict_results[EMcolname])\n    dict_results[\"lang\"].append('avg')\n    dict_results[F1colname].append(avg_f1)\n    dict_results[EMcolname].append(avg_em)\n    df_results = pd.DataFrame(dict_results).round(2)\n    return df_results","metadata":{"execution":{"iopub.status.busy":"2024-10-15T15:19:02.049119Z","iopub.execute_input":"2024-10-15T15:19:02.049518Z","iopub.status.idle":"2024-10-15T15:19:02.057742Z","shell.execute_reply.started":"2024-10-15T15:19:02.049478Z","shell.execute_reply":"2024-10-15T15:19:02.056748Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"\ndf_results_zero_shot_mbert = results_df(results_zero_shot_mbert, \"Zero-shot mBERT\")\ndf_results_zero_shot_mbert.to_csv(\"results_zero_shot_mbert.csv\")\ndf_results_zero_shot_mbert","metadata":{"execution":{"iopub.status.busy":"2024-10-15T15:19:06.983421Z","iopub.execute_input":"2024-10-15T15:19:06.983822Z","iopub.status.idle":"2024-10-15T15:19:07.015056Z","shell.execute_reply.started":"2024-10-15T15:19:06.983782Z","shell.execute_reply":"2024-10-15T15:19:07.013989Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"     lang  F1_Zero-shot mBERT  EM_Zero-shot mBERT\n0   de.de               59.39               43.75\n1   de.en               62.36               46.73\n2   de.es               56.37               40.99\n3   de.hi               34.05               21.54\n4   en.de               66.43               52.47\n5   en.en               80.30               67.02\n6   en.es               67.38               52.75\n7   en.hi               39.27               26.27\n8   es.de               60.59               40.20\n9   es.en               66.93               46.39\n10  es.es               64.88               43.61\n11  es.hi               36.17               20.08\n12  hi.de               47.55               33.85\n13  hi.en               52.90               37.11\n14  hi.es               43.73               29.08\n15  hi.hi               46.21               30.01\n16    avg               55.28               39.49","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lang</th>\n      <th>F1_Zero-shot mBERT</th>\n      <th>EM_Zero-shot mBERT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>de.de</td>\n      <td>59.39</td>\n      <td>43.75</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>de.en</td>\n      <td>62.36</td>\n      <td>46.73</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>de.es</td>\n      <td>56.37</td>\n      <td>40.99</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>de.hi</td>\n      <td>34.05</td>\n      <td>21.54</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>en.de</td>\n      <td>66.43</td>\n      <td>52.47</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>en.en</td>\n      <td>80.30</td>\n      <td>67.02</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>en.es</td>\n      <td>67.38</td>\n      <td>52.75</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>en.hi</td>\n      <td>39.27</td>\n      <td>26.27</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>es.de</td>\n      <td>60.59</td>\n      <td>40.20</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>es.en</td>\n      <td>66.93</td>\n      <td>46.39</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>es.es</td>\n      <td>64.88</td>\n      <td>43.61</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>es.hi</td>\n      <td>36.17</td>\n      <td>20.08</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>hi.de</td>\n      <td>47.55</td>\n      <td>33.85</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>hi.en</td>\n      <td>52.90</td>\n      <td>37.11</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>hi.es</td>\n      <td>43.73</td>\n      <td>29.08</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>hi.hi</td>\n      <td>46.21</td>\n      <td>30.01</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>avg</td>\n      <td>55.28</td>\n      <td>39.49</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments, Trainer, DefaultDataCollator\n\nmodel_name = \"vanichandna/xlm-roberta-finetuned-squad\"\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name,from_tf=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndata_collator = DefaultDataCollator()\n\nbatch_size = 16\ntraining_args = TrainingArguments(\n    output_dir=\"xlm-roberta-finetuned-squad\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=None,\n    eval_dataset=None,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T15:19:10.937842Z","iopub.execute_input":"2024-10-15T15:19:10.938242Z","iopub.status.idle":"2024-10-15T15:21:33.570316Z","shell.execute_reply.started":"2024-10-15T15:19:10.938202Z","shell.execute_reply":"2024-10-15T15:21:33.569227Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3191d3237a7c4d0b9cf19699da49a3de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tf_model.h5:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0ad3d6a47b3452db8e1770acf3ba07d"}},"metadata":{}},{"name":"stderr","text":"All TF 2.0 model weights were used when initializing XLMRobertaForQuestionAnswering.\n\nAll the weights of XLMRobertaForQuestionAnswering were initialized from the TF 2.0 model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/398 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"834f9b86545948a59f1df8d273bb533b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d289306f239b4c298d7a8a691a0e1487"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f23d5cb5ca34e3a9105d3da754063da"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"split = \"test\"\nmap_datasets(langs_test, split, prepare_validation_features)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T15:22:36.446892Z","iopub.execute_input":"2024-10-15T15:22:36.447285Z","iopub.status.idle":"2024-10-15T15:23:51.276871Z","shell.execute_reply.started":"2024-10-15T15:22:36.447249Z","shell.execute_reply":"2024-10-15T15:23:51.275891Z"},"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4517 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80d45ed7873049dd951a96fe7c0ea0aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4517 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"448085b18ffb45ceae4e17a8da19e16d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1776 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94ffdc2d26bb4a8886c530de3d38fc81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1430 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c8470d0fcdf4a5385f08e6a99076fff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4517 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4af2f9deef1a47858313a85b268beb3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11590 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da3741d3f7034f3cacc6ded9697ffcde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5253 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a42a733e4eb44f99ede9b793f568dae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4918 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c6aaf1c79b742cc83c9906f66bd6732"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1776 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f409b7df32e417ea73c1bbc0356df74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5253 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0298d508be964766b2b69b22a304433a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5253 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b73ab8fc59de437fb07afd0f94c0b4ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1723 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50876f0f3d6140c0b46ef410b407afb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1430 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ce6bf119f1d43238bf87b6f64a35e49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4918 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc34bae99468440192a06448895260d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1723 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc991f433ed14dad82e8ce9f61f2ac88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4918 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d69429b04f445debebd683242edacae"}},"metadata":{}}]},{"cell_type":"code","source":"results_zero_shot_xlm_r = compute_results(langs_test, split)\nprint(results_zero_shot_xlm_r)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T15:25:06.779310Z","iopub.execute_input":"2024-10-15T15:25:06.779712Z","iopub.status.idle":"2024-10-15T15:46:59.542606Z","shell.execute_reply.started":"2024-10-15T15:25:06.779675Z","shell.execute_reply":"2024-10-15T15:46:59.539960Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4517 example predictions split into 5278 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4517 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceba0455f7714a139970ebd2be4b4f48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4517 example predictions split into 5274 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4517 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"597a9190afc249849b43ea92bc284dd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1776 example predictions split into 2066 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1776 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec97812be6a1453097c7057d45a78851"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1430 example predictions split into 1604 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8038abcfba9649609292313bf8e89a11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4517 example predictions split into 5759 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4517 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"486bedc014e5458eaeebf3f28e7fdbd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 11590 example predictions split into 15269 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/11590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaf501850b8a4a0ebe041d7d94024097"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 5253 example predictions split into 6840 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5253 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0136f23312643a890bb0c5a305195a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4918 example predictions split into 6517 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4918 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"044c79418a414a01904eeeb9c88e1267"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1776 example predictions split into 1837 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1776 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16ca129dfc584a948409d589ae7ff36a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 5253 example predictions split into 5454 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5253 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7be6671b0294dc19f270940bcaa9c12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 5253 example predictions split into 5457 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5253 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c289f80a5ea4e57a4f3f8494dc47869"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1723 example predictions split into 1783 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1723 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"996d0ec238d84ff6a5e760aca6d787b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1430 example predictions split into 1760 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e362a05d406042b88fb9ed87e9e7402e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4918 example predictions split into 6319 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4918 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"369fd826e21a4c5299c28a5d813e6ab7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1723 example predictions split into 2148 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1723 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceef259ff9dd461cadde41c2dcf9d081"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4918 example predictions split into 6351 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4918 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee0e9c580744ae8b19073ccc7ddde4d"}},"metadata":{}},{"name":"stdout","text":"{'de.de': {'exact_match': 46.734558335178214, 'f1': 62.19529747175471}, 'de.en': {'exact_match': 44.2993137037857, 'f1': 59.94603497539026}, 'de.es': {'exact_match': 29.72972972972973, 'f1': 43.999710609466405}, 'de.hi': {'exact_match': 17.622377622377623, 'f1': 29.40625968950811}, 'en.de': {'exact_match': 47.13305291122426, 'f1': 60.74856919410398}, 'en.en': {'exact_match': 68.01553062985332, 'f1': 80.77800829427684}, 'en.es': {'exact_match': 43.91776127926899, 'f1': 57.840349958785666}, 'en.hi': {'exact_match': 32.025213501423345, 'f1': 44.96658524558857}, 'es.de': {'exact_match': 32.601351351351354, 'f1': 50.50384084877522}, 'es.en': {'exact_match': 45.07900247477632, 'f1': 65.96264928681461}, 'es.es': {'exact_match': 46.087949743003996, 'f1': 66.5320395240994}, 'es.hi': {'exact_match': 17.06326175275682, 'f1': 31.808537328463554}, 'hi.de': {'exact_match': 27.832167832167833, 'f1': 42.796510642509226}, 'hi.en': {'exact_match': 43.411956079707195, 'f1': 60.58569483108654}, 'hi.es': {'exact_match': 23.041207196749856, 'f1': 37.39184285657846}, 'hi.hi': {'exact_match': 44.225294835298904, 'f1': 61.347488336466846}}\n","output_type":"stream"}]},{"cell_type":"code","source":"df_results_zero_shot_xlm_r = results_df(results_zero_shot_xlm_r, \"Zero-shot XML-R\")\ndf_results_zero_shot_xlm_r.to_csv(\"results_zero_shot_xlm_r.csv\")\ndf_results_zero_shot_xlm_r","metadata":{"execution":{"iopub.status.busy":"2024-10-15T15:49:46.302267Z","iopub.execute_input":"2024-10-15T15:49:46.302675Z","iopub.status.idle":"2024-10-15T15:49:46.319329Z","shell.execute_reply.started":"2024-10-15T15:49:46.302637Z","shell.execute_reply":"2024-10-15T15:49:46.318488Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"     lang  F1_Zero-shot XML-R  EM_Zero-shot XML-R\n0   de.de               62.20               46.73\n1   de.en               59.95               44.30\n2   de.es               44.00               29.73\n3   de.hi               29.41               17.62\n4   en.de               60.75               47.13\n5   en.en               80.78               68.02\n6   en.es               57.84               43.92\n7   en.hi               44.97               32.03\n8   es.de               50.50               32.60\n9   es.en               65.96               45.08\n10  es.es               66.53               46.09\n11  es.hi               31.81               17.06\n12  hi.de               42.80               27.83\n13  hi.en               60.59               43.41\n14  hi.es               37.39               23.04\n15  hi.hi               61.35               44.23\n16    avg               53.55               38.05","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lang</th>\n      <th>F1_Zero-shot XML-R</th>\n      <th>EM_Zero-shot XML-R</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>de.de</td>\n      <td>62.20</td>\n      <td>46.73</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>de.en</td>\n      <td>59.95</td>\n      <td>44.30</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>de.es</td>\n      <td>44.00</td>\n      <td>29.73</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>de.hi</td>\n      <td>29.41</td>\n      <td>17.62</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>en.de</td>\n      <td>60.75</td>\n      <td>47.13</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>en.en</td>\n      <td>80.78</td>\n      <td>68.02</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>en.es</td>\n      <td>57.84</td>\n      <td>43.92</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>en.hi</td>\n      <td>44.97</td>\n      <td>32.03</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>es.de</td>\n      <td>50.50</td>\n      <td>32.60</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>es.en</td>\n      <td>65.96</td>\n      <td>45.08</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>es.es</td>\n      <td>66.53</td>\n      <td>46.09</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>es.hi</td>\n      <td>31.81</td>\n      <td>17.06</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>hi.de</td>\n      <td>42.80</td>\n      <td>27.83</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>hi.en</td>\n      <td>60.59</td>\n      <td>43.41</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>hi.es</td>\n      <td>37.39</td>\n      <td>23.04</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>hi.hi</td>\n      <td>61.35</td>\n      <td>44.23</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>avg</td>\n      <td>53.55</td>\n      <td>38.05</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments, Trainer, DefaultDataCollator\n\nmodel_name = \"Palak/xlm-roberta-large_squad\"\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndata_collator = DefaultDataCollator()\n\nbatch_size = 16\ntraining_args = TrainingArguments(\n    output_dir=\"xlm-roberta-large_squad\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=None,\n    eval_dataset=None,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T15:49:49.445778Z","iopub.execute_input":"2024-10-15T15:49:49.446189Z","iopub.status.idle":"2024-10-15T15:51:01.673465Z","shell.execute_reply.started":"2024-10-15T15:49:49.446148Z","shell.execute_reply":"2024-10-15T15:51:01.672157Z"},"trusted":true},"execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40191bae6ea741e79da9b079059cfaee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"334666afb72b40e2b4c21b79fb59a202"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1736f3f8658e4b73a6b30effa52477d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ad9a6d08edb4d7bb05c0f599ecc8863"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7365651a1fa14207b9a9811d39835bb9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 19\u001b[0m\n\u001b[1;32m      8\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m      9\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     10\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm-roberta-large_squad\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:554\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    553\u001b[0m ):\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:802\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 802\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2958\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2954\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2955\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2956\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2957\u001b[0m         )\n\u001b[0;32m-> 2958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 787.12 MiB is free. Process 2624 has 15.12 GiB memory in use. Of the allocated memory 1.04 GiB is allocated by PyTorch, and 88.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 787.12 MiB is free. Process 2624 has 15.12 GiB memory in use. Of the allocated memory 1.04 GiB is allocated by PyTorch, and 88.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}]},{"cell_type":"code","source":"split = \"test\"\nmap_datasets(langs_test, split, prepare_validation_features)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T15:52:45.421364Z","iopub.execute_input":"2024-10-15T15:52:45.422129Z","iopub.status.idle":"2024-10-15T15:54:01.007344Z","shell.execute_reply.started":"2024-10-15T15:52:45.422087Z","shell.execute_reply":"2024-10-15T15:54:01.006327Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4517 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bf68d4f601b4c06baa315b01132fd3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4517 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d85438a6d9842de82aa03a796e6bd01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1776 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b194bd0ba8bf4022b1537f33a7dffb9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1430 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a44cbd95d8a1469793c837d8f094590d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4517 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f15829b40524f07bb12b924f4c759ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11590 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72eed791a06244daa122b3495cbe4ae1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5253 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec44d72cdee64414984844c93e37d030"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4918 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"075626a10f024bf5b624429101738e95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1776 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c8f64f9d1e84fe39f117f2dfacdb2de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5253 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5df577ee885438aa58e850ccca544c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5253 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14099c2667514932bc20d23efe005eac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1723 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bb15864abdf4de8823825176e937369"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1430 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e27a6d8eca54a3aa724816cb00a7dbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4918 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9792e075ddc245cb95fb8ac14b641a4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1723 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"668f52f9af4b4be0b39ab487fd768f05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4918 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34f11c566f734a3fae20ce3aac357fe1"}},"metadata":{}}]},{"cell_type":"code","source":"results_zero_shot_xlm_r_large = compute_results(langs_test, split)\nprint(results_zero_shot_xlm_r_large)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T15:54:36.209616Z","iopub.execute_input":"2024-10-15T15:54:36.210503Z","iopub.status.idle":"2024-10-15T16:16:17.258215Z","shell.execute_reply.started":"2024-10-15T15:54:36.210462Z","shell.execute_reply":"2024-10-15T16:16:17.257278Z"},"trusted":true},"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4517 example predictions split into 5278 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4517 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbb8215995c14f7699b70ef58eff7fb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4517 example predictions split into 5274 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4517 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3627e0bff26948f8a2cdd8cbe82878a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1776 example predictions split into 2066 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1776 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40d1c53101ed4d8583ef5503e873ac2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1430 example predictions split into 1604 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63cbc324bac9444bbd209b199288cdd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4517 example predictions split into 5759 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4517 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73d29a902ea944c094b39789ddefcc92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 11590 example predictions split into 15269 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/11590 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d93d7386f6b41d5a28a83641a81cdf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 5253 example predictions split into 6840 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5253 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc8c264d4ece47b5a73fc9263c46bb0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4918 example predictions split into 6517 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4918 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8e9efe4c45849e5b129880ac65f0c4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1776 example predictions split into 1837 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1776 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0607b463669493cb734830115c4469a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 5253 example predictions split into 5454 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5253 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db9959c827394930bf7c64136445abf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 5253 example predictions split into 5457 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5253 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c66220a648748bcbf474565f5070197"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1723 example predictions split into 1783 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1723 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"714c064abec9471799b893bf3c6248ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1430 example predictions split into 1760 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1430 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3765c7a166c54bf885b105b18492f206"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4918 example predictions split into 6319 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4918 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6fb4fae9f2d414ea003e6c141651938"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 1723 example predictions split into 2148 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1723 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cac0c5d993a411187f21eb6e36b8cfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Post-processing 4918 example predictions split into 6351 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4918 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13f6906a0a3047c4a57465cc934a738b"}},"metadata":{}},{"name":"stdout","text":"{'de.de': {'exact_match': 46.734558335178214, 'f1': 62.19529747175471}, 'de.en': {'exact_match': 44.2993137037857, 'f1': 59.94603497539026}, 'de.es': {'exact_match': 29.72972972972973, 'f1': 43.999710609466405}, 'de.hi': {'exact_match': 17.622377622377623, 'f1': 29.40625968950811}, 'en.de': {'exact_match': 47.13305291122426, 'f1': 60.74856919410398}, 'en.en': {'exact_match': 68.01553062985332, 'f1': 80.77800829427684}, 'en.es': {'exact_match': 43.91776127926899, 'f1': 57.840349958785666}, 'en.hi': {'exact_match': 32.025213501423345, 'f1': 44.96658524558857}, 'es.de': {'exact_match': 32.601351351351354, 'f1': 50.50384084877522}, 'es.en': {'exact_match': 45.07900247477632, 'f1': 65.96264928681461}, 'es.es': {'exact_match': 46.087949743003996, 'f1': 66.5320395240994}, 'es.hi': {'exact_match': 17.06326175275682, 'f1': 31.808537328463554}, 'hi.de': {'exact_match': 27.832167832167833, 'f1': 42.796510642509226}, 'hi.en': {'exact_match': 43.411956079707195, 'f1': 60.58569483108654}, 'hi.es': {'exact_match': 23.041207196749856, 'f1': 37.39184285657846}, 'hi.hi': {'exact_match': 44.225294835298904, 'f1': 61.347488336466846}}\n","output_type":"stream"}]},{"cell_type":"code","source":"df_results_zero_shot_xlm_r_large = results_df(results_zero_shot_xlm_r_large, \"Zero-shot XML-R Large\")\ndf_results_zero_shot_xlm_r_large.to_csv(\"results_zero_shot_xlm_r_large.csv\")\ndf_results_zero_shot_xlm_r_large","metadata":{"execution":{"iopub.status.busy":"2024-10-15T16:16:33.888314Z","iopub.execute_input":"2024-10-15T16:16:33.888972Z","iopub.status.idle":"2024-10-15T16:16:33.905661Z","shell.execute_reply.started":"2024-10-15T16:16:33.888923Z","shell.execute_reply":"2024-10-15T16:16:33.904726Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"     lang  F1_Zero-shot XML-R Large  EM_Zero-shot XML-R Large\n0   de.de                     62.20                     46.73\n1   de.en                     59.95                     44.30\n2   de.es                     44.00                     29.73\n3   de.hi                     29.41                     17.62\n4   en.de                     60.75                     47.13\n5   en.en                     80.78                     68.02\n6   en.es                     57.84                     43.92\n7   en.hi                     44.97                     32.03\n8   es.de                     50.50                     32.60\n9   es.en                     65.96                     45.08\n10  es.es                     66.53                     46.09\n11  es.hi                     31.81                     17.06\n12  hi.de                     42.80                     27.83\n13  hi.en                     60.59                     43.41\n14  hi.es                     37.39                     23.04\n15  hi.hi                     61.35                     44.23\n16    avg                     53.55                     38.05","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lang</th>\n      <th>F1_Zero-shot XML-R Large</th>\n      <th>EM_Zero-shot XML-R Large</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>de.de</td>\n      <td>62.20</td>\n      <td>46.73</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>de.en</td>\n      <td>59.95</td>\n      <td>44.30</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>de.es</td>\n      <td>44.00</td>\n      <td>29.73</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>de.hi</td>\n      <td>29.41</td>\n      <td>17.62</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>en.de</td>\n      <td>60.75</td>\n      <td>47.13</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>en.en</td>\n      <td>80.78</td>\n      <td>68.02</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>en.es</td>\n      <td>57.84</td>\n      <td>43.92</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>en.hi</td>\n      <td>44.97</td>\n      <td>32.03</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>es.de</td>\n      <td>50.50</td>\n      <td>32.60</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>es.en</td>\n      <td>65.96</td>\n      <td>45.08</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>es.es</td>\n      <td>66.53</td>\n      <td>46.09</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>es.hi</td>\n      <td>31.81</td>\n      <td>17.06</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>hi.de</td>\n      <td>42.80</td>\n      <td>27.83</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>hi.en</td>\n      <td>60.59</td>\n      <td>43.41</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>hi.es</td>\n      <td>37.39</td>\n      <td>23.04</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>hi.hi</td>\n      <td>61.35</td>\n      <td>44.23</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>avg</td>\n      <td>53.55</td>\n      <td>38.05</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}